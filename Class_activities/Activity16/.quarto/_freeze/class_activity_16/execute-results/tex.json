{
  "hash": "dddece985d7da82c2956401bb199ca13",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Class Activity 16\"\nauthor: \"Your name here\"\ndate: \" April 30 2024\"\nformat:\n  pdf: default\n  html:\n    df_print: paged\neditor: visual\nexecute: \n  eval: true\n  echo: true\n  warning: false\n  error: true\n---\n\n\n\n\n\n\n\n\\vspace*{1in}\n\n\n## Group Activity 1\n\n\n### a. Scrape the first table in `List_of_NASA_missions` wiki page. Additionally, use `janitor::clean_names()` to clean the column names and store the resulting table as `NASA_missions.csv` in your working folder.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwiki_NASA <- \"https://en.wikipedia.org/wiki/List_of_NASA_missions\"\n\n# Scrape the data and write the first table to a CSV file\nbow(wiki_NASA) %>% \n  scrape() %>% \n  \n```\n\n::: {.cell-output .cell-output-error}\n\n```\nError: <text>:7:0: unexpected end of input\n5:   scrape() %>% \n6:   \n  ^\n```\n\n\n:::\n:::\n\n\n\n\n\n### b. Now, write a code snippet to scrape all the URLs from the anchor tags (<a>) on a given Wikipedia page, convert the relative URLs to absolute URLs, and store the results in a tibble and save it as `NASA_missions_urls.csv` in your working folder.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Scrape the data and write the URLs to a CSV file\nbow(wiki_NASA) %>% \n  scrape() %>% \n \n```\n\n::: {.cell-output .cell-output-error}\n\n```\nError: <text>:5:0: unexpected end of input\n3:   scrape() %>% \n4:  \n  ^\n```\n\n\n:::\n:::\n\n\n\n\n\n\n## Group Activity 2\n\n### a. How do you scrape a table from a [web page](https://finance.yahoo.com/quote/CL%3DF/history?p=CL%3DF) using `rvest`? After scraping the data, clean the column names with `janitor`, and prepare the data for analysis in R?\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nyf <- \"https://finance.yahoo.com/quote/CL%3DF/history?p=CL%3DF\"\n  bow(yf) %>% scrape() %>% \n   \n```\n\n::: {.cell-output .cell-output-error}\n\n```\nError: <text>:4:0: unexpected end of input\n2:   bow(yf) %>% scrape() %>% \n3:    \n  ^\n```\n\n\n:::\n:::\n\n\n\n\n\n### b.  Write the R code to create a time trend plot of opening prices from the scraped data using `ggplot2.`\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(, aes(x = , y = )) +\n  geom_line() + \n  geom_point() + \n  scale_x_date(date_labels = \"%b %d, %Y\", date_breaks = \"1 week\") + \n  labs(title = \"Time Trend of Opening Prices\", x = \"Date\", y = \"Opening Price\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) \n```\n\n::: {.cell-output-display}\n![](class_activity_16_files/figure-pdf/unnamed-chunk-4-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\n\n### c. How can you transform the data into a long format suitable for plotting multiple price types with `ggplot2`?\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nticker_long <- ticker %>%\n  pivot_longer(cols = , names_to = , values_to = )\n```\n\n::: {.cell-output .cell-output-error}\n\n```\nError in eval(expr, envir, enclos): object 'ticker' not found\n```\n\n\n:::\n:::\n\n\n\n\n\n### d. Show how to create a `ggplot2` visualization that includes lines and points, with different colors and shapes for each price type, and make the x-axis dates legible.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(ticker_long, aes(x = , y = , color = )) +\n  geom_line() +\n  geom_point(aes(shape = ), size = 2) + # Different shapes for each price type\n  scale_color_manual(values = c(\"open\" = \"blue\", \"close\" = \"green\", \"adj_close\" = \"red\")) +\n  scale_x_date(date_labels = \"%b %d, %Y\", date_breaks = \"1 week\") + \n  labs(title = \"Time Trend of Stock Prices\", x = \"Date\", y = \"Price\") +\n  theme_minimal() +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    legend.position = \"bottom\"\n  ) +\n  guides(shape = guide_legend(title = \"Price Type\"), color = guide_legend(title = \"Price Type\"))\n```\n\n::: {.cell-output .cell-output-error}\n\n```\nError in eval(expr, envir, enclos): object 'ticker_long' not found\n```\n\n\n:::\n:::\n\n\n\n\n\n# Group Activity 3\n\nIn this activity, you'll scrape web data using `rvest` and tidy up the results into a well-formatted table from this [web page](https://realpython.github.io/fake-jobs/). Start by extracting job titles from a given URL, then gather the associated company names, and trim any leading or trailing whitespace from the location data. Next, retrieve the posting dates and the URLs for the full job descriptions. Finally, combine all these elements into a single dataframe, ensuring that each piece of information aligns correctly. Your task is to produce a clean and informative table that could be useful for job seekers. o facilitate the selection of the correct CSS selectors, you may find the `SelectorGadget` Chrome extension particularly useful.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nurl <- \"https://realpython.github.io/fake-jobs/\"\n\ntitle <- bow(url) %>% scrape() %>%    # part 1\ncompany <- bow(url) %>% scrape() %>%  # part 2\nlocation <- bow(url) %>% scrape() %>%  # part 3\ntime <- bow(url) %>% scrape() %>%  # part 4\nhtml <- bow(url) %>% scrape() %>%    # part 5\n\n# Create a dataframe\ntibble(title = title, company = company, location = location, time = time, html = html) # part 6\n```\n\n::: {.cell-output .cell-output-error}\n\n```\nError in `tibble()`:\n! All columns in a tibble must be vectors.\nx Column `.` is a `xml_document/xml_node` object.\n```\n\n\n:::\n:::\n",
    "supporting": [
      "class_activity_16_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": null,
    "postProcess": false
  }
}