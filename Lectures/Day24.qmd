---
title: "Logistic Regression"
title-slide-attributes:
  data-background-image: images/lake.jpg
  data-background-size: contain
  data-background-opacity: "0.5"
subtitle: "STAT 220"
author: "Bastola"
format:
  revealjs: 
    theme: [default, scss/main.scss]
    slide_level: 2
    slide-number: true
    preview-links: auto
    history: true
    chalkboard: true
    transition: slide
    background-transition: fade    
    touch: false
    controls: true
filters:
  - shinylive
  - webr
---


```{r setup, include=FALSE}
# load necessary packages
library(tidyr)
library(dplyr)
library(ggplot2)
library(countdown)
library(ggthemes)
library(tidyverse)
library(stringr)
library(flipbookr)
library(htmlwidgets)
library(lubridate)
library(palmerpenguins)
library(fontawesome)
library(class)
library(patchwork)
library(tidymodels)
library(mlbench)     # for PimaIndiansDiabetes2 dataset
library(janitor)
library(parsnip)
library(kknn)
library(paletteer)
library(corrr)
library(scico)
library(gridExtra)
library(yardstick)

select <- dplyr::select

# Set ggplot theme
# theme_set(theme_stata(base_size = 10))

yt <- 0

standardize <- function(x, na.rm = FALSE) {
  (x - mean(x, na.rm = na.rm)) / sd(x, na.rm = na.rm)
}

# Load the fire data
fire <- read_csv("https://raw.githubusercontent.com/deepbas/statdatasets/main/Algeriafires.csv")
fire <- fire %>% clean_names() %>% na.omit() %>% mutate_at(c(10,13), as.numeric)
fire1 <- fire %>% mutate(across(where(is.numeric), standardize))

fire_raw <- fire %>% select(temperature, isi, classes)


fire1 <- fire %>% 
  mutate(across(where(is.numeric), standardize)) %>% 
  mutate(classes = as.factor(classes))


fire_recipe <- recipe(classes ~ ., data = fire_raw) %>%
 step_scale(all_predictors()) %>%
 step_center(all_predictors()) %>%
 prep()

fire_scaled <- bake(fire_recipe, fire_raw)

fire_knn_spec <- nearest_neighbor(mode = "classification",
                             engine = "kknn",
                             weight_func = "rectangular",
                             neighbors = 5)

fire_knn_fit <- fire_knn_spec %>%
 fit(classes ~ ., data = fire_scaled)


data(PimaIndiansDiabetes2)
db <- PimaIndiansDiabetes2
db <- db %>% drop_na()  %>% 
  mutate(diabetes = fct_relevel(diabetes, c("neg", "pos"))) # Relevels 'diabetes' factor to ensure 'neg' comes before 'pos'
```




## Classification using simple logistic regression

::: cle

- [Goal:]{.bold-blue} Understand why logistic regression is used for categorical outcomes.
- [Context:]{.bold-blue} Traditional linear regression is limited to continuous outcomes.
- [Solution:]{.bold-blue} Logistic regression models the probability of a categorical outcome.

:::


## Why Not Linear Regression?

::: cle

- [Issue with categorical outcomes:]{.bold-green} Predictions can fall outside the $[0,1]$ range.
- [Consequence:]{.bold-green} Nonsensical probability predictions for binary outcomes $(Y=0\text{ or }Y=1)$).

:::


## Introduction to Logistic Regression

::: cle

- [Solution:]{.bold-blue} Transform the outcome into a format that linear regression can handle.
- [Method:]{.bold-blue} Model the log odds of the probability of success versus failure.

:::

## From Probability to Odds

::: cle
::: font80

- [Probability:]{.bold-blue} Chance of success ($Y=1$) over total possibilities.
$$P(Y=1)=\frac{\text { number of successes }}{\text { total trials }}$$
- [Odds:]{.bold-blue} Ratio of the probability of success to the probability of failure.
$$Odds =\frac{P(Y=1)}{1-P(Y=1)}$$
:::
:::

## Logit Transformation

::: cle
::: font90

- [Goal:]{.bold-blue} Convert odds to a continuous scale that can span negative to positive infinity.
- [Logit Function:]{.bold-blue} Natural logarithm of the odds.
$$\log \left(\frac{P(Y=1)}{1-P(Y=1)}\right)$$
- Why? Makes it possible to use linear regression techniques.

:::
:::

##

::: cle
::: panel-tabset

### LR 

```{r, echo=FALSE, fig.width=6, fig.height=4.5, fig.align='center', out.width = "60%"}
db_plot <- db %>% mutate(y = ifelse(diabetes == "pos", 1, 0))
ggplot(db_plot, aes(x=glucose, y=y)) + 
  geom_jitter(aes(color=diabetes), width = 0.1, height = 0.01, shape = 16) +  # Adds jitter to points for better visibility
  geom_smooth(method = glm, method.args = list(family = binomial), se = FALSE, color = "black") +  # Logistic regression line
  labs(y = expression(paste("Probability of diabetes, ", p == P(Y == 1))), 
       title = "Logistic regression probability of diabetes given glucose")+
  theme_tufte()+
  scale_color_wsj()
```


### LR Summary


::: font80
- Binary response, $Y$,  with an explanatory (predictor, features) variables, $X_1$.
- We model the probability that $Y$ belongs to a particular category.


$$P(Y = 1 ) = \frac{e^{\beta_0 + \beta_1X_1}}{1 + e^{\beta_0 + \beta_1X_1}}$$

$$\text{Odds} = \frac{P(Y = 1 )}{1 - P(Y = 1 )} = e^{\beta_0 + \beta_1X_1}$$

$$\text{Log Odds} = \beta_0 + \beta_1X_1$$
:::


### Data Preparation

```{r}
# Create data split for train and test
set.seed(12345) # Ensures reproducibility of the data split
db_single <- db %>% 
  select(diabetes, glucose) %>%  
  # Relevels 'diabetes' factor to ensure 'neg' comes before 'pos'
  mutate(diabetes = fct_relevel(diabetes, c("neg", "pos")))  
db_split <- initial_split(db_single, prop = 0.80) 
db_train <- db_split %>% training()
db_test <- db_split %>% testing() 
```



### Modeling

```{r}
set.seed(12345) 
fitted_logistic_model <- logistic_reg(engine = "glm",  
                                      mode = "classification") %>% 
  fit(diabetes~., data = db_train)  
```

:::
:::


## Tidy the Summary

::: zen

```{r}
broom::tidy(fitted_logistic_model)
```



$$\log \left(\frac{p}{1-p}\right)=-5.61+0.0392 \cdot \text { glucose }$$

Where $p = P(Y = 1)$ is the probability of having diabetes 


:::

::: middle-align-xy

## Interpreting Coefficients: Log Odds

::: vvt

- Coefficient Meaning: Change in log odds of the outcome for a one-unit increase in the predictor.


A coefficient of 0.0392 for glucose means...

...a one-unit increase in glucose level increases the log odds of diabetes by 0.0392.

:::
:::

## Exponentiating Coefficients: Odds Ratios

::: vvt
::: font80

$$Odds = \frac{probability}{1 - probability}$$


```{r}
broom::tidy(fitted_logistic_model, exponentiate = TRUE)
```



$$\text { Odds }=0.00364 \times(1.04)^{\text {glucose }}$$


An odds ratio of 1.04 means that for each additional unit of glucose, the odds of having diabetes increase by 4%.

:::
:::

## Understanding Odds Ratios: Summary

::: cle

- [Transformation:]{.bold-blue} Turning log-odds coefficients into odds ratios.
- [Odds Ratio $=e^{\beta_i}$]{.bold-blue}
- [Interpretation:]{.bold-blue} For each unit increase in the predictor, the odds multiply by the odds ratio.

:::

## Importance of Scaling and Centering

```r
db_recipe <- recipe(diabetes ~ ., data = db_train) %>% 
  step_scale(all_numeric_predictors()) %>% 
  step_center(all_numeric_predictors()) %>%   
  prep()
```

::: cle

- Ensures all features contribute equally to model calculations.
- Helps gradient descent algorithms converge more quickly.
- Standardizes coefficient scales, improving model interpretability.

:::

::: centered-content

## <i class="fa fa-pencil-square-o" style="font-size:48px;color:purple"> Group Activity `r (yt <- yt + 1)`</i> {background="#ffdf9e"}

::: row
::: left
![](https://media.giphy.com/media/RKApDdwsQ6jkwd6RNn/giphy.gif)
:::

::: right

<br>

::: lqt

- Please clone the `ca24-yourusername` repository from  [Github](https://github.com/DataScienceSpring24)
- Please do problem 1 in the class activity for today
:::
:::
:::

`r countdown(minutes = 10, seconds = 00, top = 0 , left = 0, color_background = "inherit", padding = "3px 4px", font_size = "1em")`
:::


## Threshold for classification

```{r, echo= FALSE, fig.width=6, fig.height=4.5, fig.align='center', out.width = "60%"}
set.seed(12345)
t <- 0.5
x.thres <- (log(t / (1 - t)) - fitted_logistic_model$fit$coefficients[1]) / fitted_logistic_model$fit$coefficients[2]

db_plot <- db %>% mutate(y = ifelse(diabetes == "pos", 1, 0))

ggplot(db_plot, aes(x = glucose, y = y)) + 
  geom_jitter(aes(color = diabetes), width = 0.1, height = 0.01, shape = 16) + 
  geom_smooth(method = glm, method.args = list(family = binomial), se = FALSE, color = "black") + 
  labs(y = expression(paste("Probability of diabetes, ", p == P(Y == 1))), 
       x = "Glucose Level", 
       title = "Probability of Diabetes Given Glucose Level") +   geom_vline(xintercept = x.thres, linetype = "dashed", color = "firebrick") + 
  annotate("text", x = x.thres + 3, y = 0.45, label = sprintf("Threshold: %.2f", x.thres), hjust = 0, color = "firebrick") + 
  annotate(geom = "rect", xmin = x.thres, xmax = 200, ymin = -.05, ymax = .05, fill = "black", alpha = 0.1) + 
  annotate(geom = "rect", xmin = 55, xmax = x.thres, ymin = 0.95, ymax = 1.05, fill = "firebrick", alpha = 0.1) + 
  theme_tufte() +
  theme(plot.title = element_text(hjust = 0.5), legend.position = "bottom") +
  xlim(c(54, 200))
```


## Class Prediction

::: eqt

```{r, fig.width=6, fig.height=4.5, fig.align='center', out.width = "50%"}
set.seed(12345)
pred_class <- predict(fitted_logistic_model,  new_data = db_test) 
bind_cols(db_test %>% select(diabetes), pred_class) %>% 
  conf_mat(diabetes, .pred_class) %>% # confusion matrix
  autoplot(type = "heatmap") # with graphics
```


:::

## [Class Probabilities with `threshold = 0.30`]{.font80}

::: eqt

```{r}
# Prediction Probabilities
library(probably)
pred_prob <- predict(fitted_logistic_model,  
                     new_data = db_test,   
                     type = "prob")

db_results <- db_test %>% bind_cols(pred_prob) %>%
  mutate(.pred_class = make_two_class_pred(.pred_neg, 
                                           levels(diabetes), 
                                           threshold = .30)) %>%
  select(diabetes, glucose, contains(".pred"))
```

```{r, echo=FALSE}
head(db_results,10)
```

:::

## Custom Metrics

::: eqt
::: row
::: left

```{r, echo=FALSE, fig.width=6, fig.height=4.5, fig.align='center', out.width = "90%"}
db_results %>%  
  conf_mat(diabetes,.pred_class) %>% 
  autoplot(type = "heatmap")
```

:::

::: right

```{r}
custom_metrics <- metric_set(accuracy, 
                             sensitivity, 
                             specificity, 
                             ppv)
custom_metrics(db_results,
               truth = diabetes,
               estimate = .pred_class)
```

:::
:::
:::

## [Class Probabilities with `threshold = 0.70`]{.font80}

::: eqt

```{r}
# Prediction Probabilities
library(probably)
pred_prob <- predict(fitted_logistic_model,  
                     new_data = db_test,   
                     type = "prob")

db_results <- db_test %>% bind_cols(pred_prob) %>%
  mutate(.pred_class = make_two_class_pred(.pred_neg, 
                                           levels(diabetes), 
                                           threshold = .70)) %>%
  select(diabetes, glucose, contains(".pred"))
```

```{r, echo=FALSE}
head(db_results,10)
```

:::

## Custom Metrics

::: eqt
::: row
::: left

```{r, echo=FALSE, fig.width=6, fig.height=4.5, fig.align='center', out.width = "90%"}
db_results %>%  
  conf_mat(diabetes,.pred_class) %>% 
  autoplot(type = "heatmap")
```

:::

::: right

```{r}
custom_metrics <- metric_set(accuracy, 
                             sensitivity, 
                             specificity, 
                             ppv)
custom_metrics(db_results,
               truth = diabetes,
               estimate = .pred_class)
```


:::
:::
:::

##

::: panel-tabset

### ROC Curve

```{r echo=FALSE}
library(yardstick)

diabetes_prob <- predict(fitted_logistic_model, db_test, type = "prob")
diabetes_results <- db_test %>% select(diabetes) %>% bind_cols(diabetes_prob)

diabetes_results %>%
  roc_curve(truth = diabetes, .pred_neg) %>%
  ggplot(aes(x = 1 - specificity, y = sensitivity)) +
  geom_line(color = "#1f77b4", size = 1.2) +
  geom_abline(linetype = "dashed", color = "gray") +
  annotate("text", x = 0.8, y = 0.1, label = paste("AUC =", round(roc_auc(diabetes_results, truth = diabetes, .pred_neg)$.estimate, 3)), hjust = 1, color = "#ff7f0e", size = 5, fontface = "bold") +
  labs(title = "ROC Curve", subtitle = "Performance of the Logistic Regression Model", x = "False Positive Rate (1 - specificity)", y = "True Positive Rate (sensitivity)") +
  theme_minimal()
```



### Code

```{r eval=FALSE}
library(yardstick)

diabetes_prob <- predict(fitted_logistic_model, db_test, type = "prob")
diabetes_results <- db_test %>% select(diabetes) %>% bind_cols(diabetes_prob)

diabetes_results %>%
  roc_curve(truth = diabetes, .pred_neg) %>%
  ggplot(aes(x = 1 - specificity, y = sensitivity)) +
  geom_line(color = "#1f77b4", size = 1.2) +
  geom_abline(linetype = "dashed", color = "gray") +
  annotate("text", x = 0.8, y = 0.1, label = paste("AUC =", round(roc_auc(diabetes_results, truth = diabetes, .pred_neg)$.estimate, 3)), hjust = 1, color = "#ff7f0e", size = 5, fontface = "bold") +
  labs(title = "ROC Curve", subtitle = "Performance of the Logistic Regression Model", x = "False Positive Rate (1 - specificity)", y = "True Positive Rate (sensitivity)") +
  theme_minimal()
```


:::


## Exact Optimal Threshold {auto-animate="true"}

::: scroll-box-20
```{r}
# Compute the ROC curve
roc_curve(diabetes_results, truth = diabetes, .pred_neg)
```
:::


## Exact Optimal Threshold {auto-animate="true"}

::: scroll-box-20
```{r}
# Compute the ROC curve
roc_curve(diabetes_results, truth = diabetes, .pred_neg) %>% 
# Look for the point where specificity is 0.87 and sensitivity is 0.76
  arrange(abs(specificity - (1 - 0.13)), abs(sensitivity - 0.76)) 
```
:::



## Exact Optimal Threshold {auto-animate="true"}

::: scroll-box-20
```{r}
# Compute the ROC curve
roc_curve(diabetes_results, truth = diabetes, .pred_neg) %>% 
# Find point where specificity is 0.87 and sensitivity is 0.76
  arrange(abs(specificity - (1 - 0.13)), abs(sensitivity - 0.76)) %>%
  slice(1)
```
:::




::: centered-content

## <i class="fa fa-pencil-square-o" style="font-size:48px;color:purple"> Group Activity `r (yt <- yt + 1)`</i> {background="#ffdf9e"}

::: row
::: left
![](https://media.giphy.com/media/RKApDdwsQ6jkwd6RNn/giphy.gif)
:::

::: right

<br>
<br>

::: lqt

- Please finish the remaining problems in the class activity for today

:::
:::
:::

`r countdown(minutes = 10, seconds = 00, top = 0 , left = 0, color_background = "inherit", padding = "3px 4px", font_size = "1em")`
:::

